2025-09-21 21:42:11 [INFO] ğŸ§¹ å‡†å¤‡è®­ç»ƒç¯å¢ƒ...
2025-09-21 21:42:12 [INFO] âœ… GPUç¼“å­˜å·²æ¸…ç†
2025-09-21 21:42:12 [INFO] ğŸš€ å¯åŠ¨è®­ç»ƒç³»ç»Ÿ...
2025-09-21 21:42:12 [INFO] ======================================================================
2025-09-21 21:42:12 [INFO] ğŸš€ TTFM 24ä»£è®­ç»ƒç³»ç»Ÿ V3 - æ¶æ„é‡æ„ç‰ˆ
2025-09-21 21:42:12 [INFO] ğŸ“ æ¨¡å‹: /mnt/d/models/åŸå§‹æ¨¡å‹/Qwen3-8B/Qwen3-8B
2025-09-21 21:42:12 [INFO] ğŸ’¾ ä¿å­˜: /mnt/d/models/ç•™å­˜æ¨¡å‹/Tigerä¿¡ä»»è®­ç»ƒ/24ä»£ç³»ç»Ÿ
2025-09-21 21:42:12 [INFO] ======================================================================
2025-09-21 21:42:12 [INFO] ğŸš€ åˆå§‹åŒ–åŸºç¡€æ¨¡å‹...
2025-09-21 21:42:12 [INFO]   æ¨¡å‹è·¯å¾„: /mnt/d/models/åŸå§‹æ¨¡å‹/Qwen3-8B/Qwen3-8B
2025-09-21 21:42:12 [INFO]   åŠ è½½è¿‡ç¨‹éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...
2025-09-21 21:42:13 [INFO] ğŸ“¦ åŠ è½½8bité‡åŒ–æ¨¡å‹...
`torch_dtype` is deprecated! Use `dtype` instead!
2025-09-21 21:42:13 [WARNING] WARNING: BNB_CUDA_VERSION=128 environment variable detected; loading libbitsandbytes_cuda128.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:21<01:24, 21.05s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:52<01:21, 27.18s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:34<01:08, 34.12s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:22<00:39, 39.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:38<00:00, 31.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:38<00:00, 31.72s/it]
Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at /mnt/d/models/åŸå§‹æ¨¡å‹/Qwen3-8B/Qwen3-8B and are newly initialized: ['model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-09-21 21:44:53 [INFO] âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ (ç¬¬1æ¬¡)
2025-09-21 21:44:53 [INFO] ğŸ“Š GPUæ˜¾å­˜: å·²åˆ†é…11.9GB / å·²é¢„ç•™23.1GB / æ€»è®¡17.1GB
2025-09-21 21:44:53 [INFO] 
======================================================================
2025-09-21 21:44:53 [INFO] ğŸ§¬ ç¬¬1/24ä»£ - æ„Ÿæƒ…åŸºç¡€å»ºè®¾
2025-09-21 21:44:53 [INFO] ======================================================================
2025-09-21 21:44:53 [INFO] ğŸ“Š ç›®æ ‡Loss: 2.00
2025-09-21 21:44:53 [INFO] 
ğŸ“¦ è®­ç»ƒæ‰¹æ¬¡: å˜ä½“[61]
2025-09-21 21:44:53 [INFO] 
ğŸ”„ è®­ç»ƒå˜ä½“ 61
2025-09-21 21:44:53 [INFO] åˆ›å»ºLoRAé€‚é…å™¨...
2025-09-21 21:44:53 [INFO]   åˆ›å»ºå…¨æ–°LoRAå±‚...
trainable params: 61,341,696 || all params: 8,252,289,024 || trainable%: 0.7433

=== ç”Ÿæˆç¬¬1ä»£æ ·æœ¬ ===
ä¸»é¢˜ï¼šèº«ä»½è®¤çŸ¥ä¸åˆå§‹ä¿¡ä»»
ä¸»è¦é¢†åŸŸï¼šèº«ä»½è®¤çŸ¥
ä¿¡ä»»åº¦ï¼š10%
æ€»æ ·æœ¬æ•°ï¼š1000
âœ“ ä¸»é¢˜æ ·æœ¬ï¼š800ä¸ª
âœ“ ä¿¡ä»»æ ·æœ¬ï¼š200ä¸ª
2025-09-21 21:44:56 [INFO]   ä½¿ç”¨1000ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
2025-09-21 21:44:56 [INFO]     å¼€å§‹tokenization 1000ä¸ªæ ·æœ¬...
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 13973.05 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 16319.49 examples/s]
2025-09-21 21:44:56 [INFO]     æ•°æ®é›†å‡†å¤‡å®Œæˆ
2025-09-21 21:44:56 [INFO]   å¼€å§‹è®­ç»ƒå˜ä½“61...
2025-09-21 21:44:56 [INFO]   å‡†å¤‡è®­ç»ƒå™¨ï¼Œæ•°æ®é›†å¤§å°: 1000
2025-09-21 21:44:56 [INFO]   ğŸ”„ å‘ç°æ£€æŸ¥ç‚¹ï¼Œä» checkpoint-30 æ¢å¤è®­ç»ƒ
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  self.setter(val)
2025-09-21 21:44:56 [INFO]   å¼€å§‹è®­ç»ƒ...
2025-09-21 21:44:56 [INFO]     æ‰¹å¤§å°: 2, æ€»æ ·æœ¬: 1000
2025-09-21 21:44:56 [INFO]     é¢„è®¡æ­¥æ•°: 500
2025-09-21 21:44:57 [INFO]   å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°: 375
  0%|          | 0/375 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/375 [00:04<28:23,  4.55s/it]  1%|          | 2/375 [00:08<24:54,  4.01s/it]  1%|          | 3/375 [00:11<23:32,  3.80s/it]  1%|          | 4/375 [00:15<23:00,  3.72s/it]  1%|â–         | 5/375 [00:18<22:35,  3.66s/it]  2%|â–         | 6/375 [00:22<22:09,  3.60s/it]  2%|â–         | 7/375 [00:25<20:32,  3.35s/it]  2%|â–         | 8/375 [00:28<20:56,  3.42s/it]  2%|â–         | 9/375 [00:32<21:11,  3.47s/it]  3%|â–         | 10/375 [00:35<21:07,  3.47s/it]You are using a model of type qwen3 to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  3%|â–         | 11/375 [00:44<30:51,  5.09s/it]  3%|â–         | 12/375 [00:48<28:26,  4.70s/it]  3%|â–         | 13/375 [00:52<27:09,  4.50s/it]  4%|â–         | 14/375 [01:25<1:18:11, 13.00s/it]  4%|â–         | 15/375 [02:07<2:11:46, 21.96s/it]  4%|â–         | 16/375 [02:46<2:42:02, 27.08s/it]  5%|â–         | 17/375 [03:30<3:11:13, 32.05s/it]  5%|â–         | 18/375 [04:18<3:39:22, 36.87s/it]  5%|â–Œ         | 19/375 [05:09<4:03:26, 41.03s/it]2025-09-21 21:51:14 [INFO]   âš¡ åŸºå‡†é€Ÿåº¦: 16.97ç§’/æ­¥
  5%|â–Œ         | 20/375 [06:17<4:51:21, 49.24s/it]You are using a model of type qwen3 to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
