2025-09-21 21:42:11 [INFO] 🧹 准备训练环境...
2025-09-21 21:42:12 [INFO] ✅ GPU缓存已清理
2025-09-21 21:42:12 [INFO] 🚀 启动训练系统...
2025-09-21 21:42:12 [INFO] ======================================================================
2025-09-21 21:42:12 [INFO] 🚀 TTFM 24代训练系统 V3 - 架构重构版
2025-09-21 21:42:12 [INFO] 📍 模型: /mnt/d/models/原始模型/Qwen3-8B/Qwen3-8B
2025-09-21 21:42:12 [INFO] 💾 保存: /mnt/d/models/留存模型/Tiger信任训练/24代系统
2025-09-21 21:42:12 [INFO] ======================================================================
2025-09-21 21:42:12 [INFO] 🚀 初始化基础模型...
2025-09-21 21:42:12 [INFO]   模型路径: /mnt/d/models/原始模型/Qwen3-8B/Qwen3-8B
2025-09-21 21:42:12 [INFO]   加载过程需要1-2分钟，请耐心等待...
2025-09-21 21:42:13 [INFO] 📦 加载8bit量化模型...
`torch_dtype` is deprecated! Use `dtype` instead!
2025-09-21 21:42:13 [WARNING] WARNING: BNB_CUDA_VERSION=128 environment variable detected; loading libbitsandbytes_cuda128.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:21<01:24, 21.05s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:52<01:21, 27.18s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:34<01:08, 34.12s/it]Loading checkpoint shards:  80%|████████  | 4/5 [02:22<00:39, 39.33s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:38<00:00, 31.05s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:38<00:00, 31.72s/it]
Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at /mnt/d/models/原始模型/Qwen3-8B/Qwen3-8B and are newly initialized: ['model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-09-21 21:44:53 [INFO] ✅ 基础模型加载完成 (第1次)
2025-09-21 21:44:53 [INFO] 📊 GPU显存: 已分配11.9GB / 已预留23.1GB / 总计17.1GB
2025-09-21 21:44:53 [INFO] 
======================================================================
2025-09-21 21:44:53 [INFO] 🧬 第1/24代 - 感情基础建设
2025-09-21 21:44:53 [INFO] ======================================================================
2025-09-21 21:44:53 [INFO] 📊 目标Loss: 2.00
2025-09-21 21:44:53 [INFO] 
📦 训练批次: 变体[61]
2025-09-21 21:44:53 [INFO] 
🔄 训练变体 61
2025-09-21 21:44:53 [INFO] 创建LoRA适配器...
2025-09-21 21:44:53 [INFO]   创建全新LoRA层...
trainable params: 61,341,696 || all params: 8,252,289,024 || trainable%: 0.7433

=== 生成第1代样本 ===
主题：身份认知与初始信任
主要领域：身份认知
信任度：10%
总样本数：1000
✓ 主题样本：800个
✓ 信任样本：200个
2025-09-21 21:44:56 [INFO]   使用1000个样本进行训练
2025-09-21 21:44:56 [INFO]     开始tokenization 1000个样本...
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 13973.05 examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 16319.49 examples/s]
2025-09-21 21:44:56 [INFO]     数据集准备完成
2025-09-21 21:44:56 [INFO]   开始训练变体61...
2025-09-21 21:44:56 [INFO]   准备训练器，数据集大小: 1000
2025-09-21 21:44:56 [INFO]   🔄 发现检查点，从 checkpoint-30 恢复训练
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  self.setter(val)
2025-09-21 21:44:56 [INFO]   开始训练...
2025-09-21 21:44:56 [INFO]     批大小: 2, 总样本: 1000
2025-09-21 21:44:56 [INFO]     预计步数: 500
2025-09-21 21:44:57 [INFO]   开始训练，总步数: 375
  0%|          | 0/375 [00:00<?, ?it/s]/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/375 [00:04<28:23,  4.55s/it]  1%|          | 2/375 [00:08<24:54,  4.01s/it]  1%|          | 3/375 [00:11<23:32,  3.80s/it]  1%|          | 4/375 [00:15<23:00,  3.72s/it]  1%|▏         | 5/375 [00:18<22:35,  3.66s/it]  2%|▏         | 6/375 [00:22<22:09,  3.60s/it]  2%|▏         | 7/375 [00:25<20:32,  3.35s/it]  2%|▏         | 8/375 [00:28<20:56,  3.42s/it]  2%|▏         | 9/375 [00:32<21:11,  3.47s/it]  3%|▎         | 10/375 [00:35<21:07,  3.47s/it]You are using a model of type qwen3 to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  3%|▎         | 11/375 [00:44<30:51,  5.09s/it]  3%|▎         | 12/375 [00:48<28:26,  4.70s/it]  3%|▎         | 13/375 [00:52<27:09,  4.50s/it]  4%|▎         | 14/375 [01:25<1:18:11, 13.00s/it]  4%|▍         | 15/375 [02:07<2:11:46, 21.96s/it]  4%|▍         | 16/375 [02:46<2:42:02, 27.08s/it]  5%|▍         | 17/375 [03:30<3:11:13, 32.05s/it]  5%|▍         | 18/375 [04:18<3:39:22, 36.87s/it]  5%|▌         | 19/375 [05:09<4:03:26, 41.03s/it]2025-09-21 21:51:14 [INFO]   ⚡ 基准速度: 16.97秒/步
  5%|▌         | 20/375 [06:17<4:51:21, 49.24s/it]You are using a model of type qwen3 to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.
/root/miniconda3/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
