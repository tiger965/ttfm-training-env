#!/usr/bin/env python3
"""
RTX 5080 8bité‡åŒ–è®­ç»ƒæµ‹è¯•è„šæœ¬
æ”¯æŒsm_120æ¶æ„ï¼Œä½¿ç”¨PyTorch nightlyå’Œbitsandbytes 0.47.0
"""

import torch
import warnings
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# å…³é”®ä¿®å¤ï¼šé˜²æ­¢PEFT dtypeè½¬æ¢é”™è¯¯
import peft.tuners.tuners_utils
peft.tuners.tuners_utils.BaseTuner._cast_adapter_dtype = lambda *args, **kwargs: None

print("=" * 60)
print("ğŸš€ RTX 5080 8bité‡åŒ–è®­ç»ƒæµ‹è¯•")
print("=" * 60)

# éªŒè¯ç¯å¢ƒ
print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"âœ… GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
print(f"âœ… æ”¯æŒæ¶æ„: {torch.cuda.get_arch_list()}")

# GPUæµ‹è¯•
x = torch.randn(1000, 1000).cuda()
y = x @ x.T
print(f"âœ… GPUè¿ç®—æµ‹è¯•: {y.shape}")
del x, y
torch.cuda.empty_cache()

# æ¨¡å‹è·¯å¾„ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
base_path = Path('/home/tiger/tiger_trust_project')
# å¦‚æœæœ‰Qwen3-8Bæ¨¡å‹ï¼Œä½¿ç”¨å®é™…è·¯å¾„
model_path = "/mnt/d/models/åŸå§‹æ¨¡å‹/Qwen3-8B/Qwen3-8B"  # æ ¹æ®æ‚¨çš„é…ç½®æ–‡ä»¶
output_dir = base_path / 'completed_models' / 'test_8bit'
output_dir.mkdir(parents=True, exist_ok=True)

print("\nğŸ“¦ å‡†å¤‡åŠ è½½8bité‡åŒ–æ¨¡å‹...")
print(f"æ¨¡å‹è·¯å¾„: {model_path}")

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
model_path_obj = Path(model_path)
if not model_path_obj.exists():
    print(f"âš ï¸ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
    print("è¯·ä¸‹è½½Qwen3-8Bæ¨¡å‹æˆ–ä¿®æ”¹è·¯å¾„")
    print("\nä½¿ç”¨ç®€å•æµ‹è¯•æ¨¡å¼...")

    # åˆ›å»ºç®€å•æµ‹è¯•
    import bitsandbytes as bnb
    print(f"âœ… bitsandbytesç‰ˆæœ¬: {bnb.__version__}")

    # æµ‹è¯•8bité‡åŒ–æ“ä½œ
    linear = bnb.nn.Linear8bitLt(100, 100).cuda()
    x = torch.randn(10, 100).cuda()
    y = linear(x)
    print(f"âœ… 8bitçº¿æ€§å±‚æµ‹è¯•: è¾“å…¥{x.shape} -> è¾“å‡º{y.shape}")

    print("\nç¯å¢ƒé…ç½®æˆåŠŸï¼éœ€è¦ä¸‹è½½æ¨¡å‹æ‰èƒ½å¼€å§‹è®­ç»ƒã€‚")
else:
    # 8bité…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
        bnb_8bit_compute_dtype=torch.float16,
        bnb_8bit_quant_type="nf4",
        bnb_8bit_use_double_quant=True,
    )

    print("â³ åŠ è½½8bité‡åŒ–æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")

    # åŠ è½½8bitæ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    )

    print("âœ… 8bitæ¨¡å‹åŠ è½½æˆåŠŸ!")

    # å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
    model = prepare_model_for_kbit_training(model)

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # LoRAé…ç½®
    print("\nğŸ¯ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # ç®€å•è®­ç»ƒæµ‹è¯•
    print("\nğŸš€ å¼€å§‹è®­ç»ƒæµ‹è¯•...")
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

    # æµ‹è¯•æ•°æ®
    test_text = "ç”¨æˆ·: ä½ å¥½\nåŠ©æ‰‹: ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚"

    # tokenize
    inputs = tokenizer(
        test_text,
        return_tensors="pt",
        max_length=64,
        truncation=True,
        padding=True
    )

    # ç§»åŠ¨åˆ°GPU
    input_ids = inputs["input_ids"].cuda()
    attention_mask = inputs["attention_mask"].cuda()

    # å‰å‘ä¼ æ’­
    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=input_ids
    )
    loss = outputs.loss

    print(f"âœ… è®­ç»ƒæŸå¤±: {loss.item():.4f}")

    # åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    print("âœ… åå‘ä¼ æ’­æˆåŠŸ!")

    # æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    memory_used = torch.cuda.memory_allocated() / 1024**3
    memory_reserved = torch.cuda.memory_reserved() / 1024**3
    print(f"\nğŸ“Š æ˜¾å­˜ä½¿ç”¨: {memory_used:.2f}GB / {memory_reserved:.2f}GB")

print("\n" + "=" * 60)
print("ğŸ‰ RTX 5080 8bitè®­ç»ƒç¯å¢ƒéªŒè¯æˆåŠŸ!")
print("æ”¯æŒsm_120æ¶æ„ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒQwen3-8B")
print("=" * 60)