#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TTFM 24ä»£è®­ç»ƒç³»ç»Ÿ V3 - æ¶æ„é‡æ„ç‰ˆ
å®Œå…¨é‡å†™çš„æ¶æ„ï¼Œæ”¯æŒçœŸæ­£çš„æ‰¹é‡å¹¶å‘è®­ç»ƒ
"""

import os
import sys
import gc
import json
import time
import math
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

# é˜²æ­¢CUDA contextè¢«forkåˆ°çˆ¶è¿›ç¨‹
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
os.environ['TOKENIZERS_PARALLELISM'] = 'true'
os.environ['OMP_NUM_THREADS'] = '8'

import torch

# æ³¨å†ŒQwen3æ¨¡å‹ - å¿…é¡»åœ¨å¯¼å…¥transformersä¹‹å‰
from transformers.models.auto import configuration_auto, modeling_auto
from transformers.models.qwen2 import Qwen2Config, Qwen2ForCausalLM
configuration_auto.CONFIG_MAPPING._extra_content['qwen3'] = Qwen2Config
modeling_auto.MODEL_FOR_CAUSAL_LM_MAPPING_NAMES['qwen3'] = 'Qwen2ForCausalLM'

# RTX 5080 å…³é”®ä¿®å¤ - å¿…é¡»åŠ åœ¨è¿™é‡Œï¼
import peft.tuners.tuners_utils
peft.tuners.tuners_utils.BaseTuner._cast_adapter_dtype = lambda *args, **kwargs: None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
)
logger = logging.getLogger(__name__)
logger.propagate = False  # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶logger

# æ¸…é™¤å·²æœ‰çš„handlersé¿å…é‡å¤
logger.handlers.clear()

# æ·»åŠ å•ä¸ªStreamHandlerç¡®ä¿å®æ—¶è¾“å‡º
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S'))
logger.addHandler(handler)

# è®¾ç½®Pythonè¾“å‡ºæ— ç¼“å†²
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 1)  # è¡Œç¼“å†²
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 1)

# å¯¼å…¥å¿…è¦çš„åº“
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training
)
from datasets import Dataset
from transformers import TrainerCallback

class SmartMemoryCallback(TrainerCallback):
    """æ™ºèƒ½æ€§èƒ½ç®¡ç† - æ¿€è¿›ä¼˜åŒ–ç‰ˆ"""

    def __init__(self):
        self.step_times = []
        self.baseline_speed = None  # ç­‰å¾…å®é™…æµ‹é‡
        self.last_cleanup = 0
        self.performance_buffer = []  # æ€§èƒ½ç¼“å†²åŒº
        self.should_restart = False
        self.cleanup_count = 0  # æ¸…ç†è®¡æ•°
        self.batch_optimized = False  # æ ‡è®°æ˜¯å¦å·²ä¼˜åŒ–batch

    def on_step_begin(self, args, state, control, **kwargs):
        self.step_start = time.time()

        # batch_size=2æ˜¯æœ€ä¼˜çš„ï¼Œä¸éœ€è¦è°ƒæ•´
        # æ˜¨å¤©çš„æ—¥å¿—è¯æ˜batch_size=2æ—¶é€Ÿåº¦3-4ç§’/æ­¥
        # batch_size=1åè€Œæ…¢ï¼ˆ6ç§’/æ­¥ï¼‰

        return control

    def on_step_end(self, args, state, control, **kwargs):
        step_time = time.time() - self.step_start
        self.step_times.append(step_time)

        # ä¿æŒæœ€è¿‘100æ­¥çš„è®°å½•
        if len(self.step_times) > 100:
            self.step_times = self.step_times[-100:]

        # å»ºç«‹åŸºå‡†é€Ÿåº¦ï¼ˆå‰20æ­¥çš„å¹³å‡å€¼ï¼Œæ›´ç¨³å®šï¼‰
        if len(self.step_times) == 20 and self.baseline_speed is None:
            # å»æ‰æœ€å¤§æœ€å°å€¼åå–å¹³å‡
            sorted_times = sorted(self.step_times[:20])
            avg_speed = np.mean(sorted_times[2:-2])  # å»æ‰å‰2ä¸ªæœ€å¿«å’Œå2ä¸ªæœ€æ…¢
            self.baseline_speed = avg_speed * 1.1  # ç•™10%å¯Œè£•
            logger.info(f"  âš¡ åŸºå‡†é€Ÿåº¦: {self.baseline_speed:.2f}ç§’/æ­¥")

        # æ™ºèƒ½æ€§èƒ½ç®¡ç† - ä»…åœ¨çœŸæ­£å¼‚å¸¸æ—¶ä»‹å…¥
        if self.baseline_speed and len(self.step_times) > 20:
            # è®¡ç®—æœ€è¿‘10æ­¥çš„å¹³å‡é€Ÿåº¦
            recent_avg = np.mean(self.step_times[-10:])

            # åªæœ‰å½“å¹³å‡é€Ÿåº¦æ˜æ˜¾ä¸‹é™æ—¶æ‰ä»‹å…¥
            if recent_avg > self.baseline_speed * 1.5:
                deviation = (recent_avg - self.baseline_speed) / self.baseline_speed * 100
                logger.warning(f"  âš ï¸ æ€§èƒ½ä¸‹é™: å¹³å‡{recent_avg:.1f}s (åå·®{deviation:.0f}%)")

                # æ¯100æ­¥æœ€å¤šæ¸…ç’†1æ¬¡ï¼Œé¿å…è¿‡åº¦ä»‹å…¥
                if self.cleanup_count < state.global_step // 100:
                    self._handle_performance_deviation(deviation, recent_avg, args, state, control)
                    self.cleanup_count += 1

        return control

    def _handle_performance_deviation(self, deviation, step_time, args, state, control):
        """å¤„ç†æ€§èƒ½åå·® - æ¸©å’Œå¹²é¢„"""

        if deviation <= 100:  # 50-100%: è½»åº¦æ¸…ç†
            logger.info(f"  ğŸ”§ è½»åº¦ä¼˜åŒ–...")
            torch.cuda.empty_cache()
            gc.collect()

        elif deviation <= 200:  # 100-200%: ä¸­åº¦æ¸…ç†
            logger.warning(f"  âš ï¸ ä¸­åº¦ä¼˜åŒ–: {step_time:.1f}s")

            # å…¨é¢æ¸…ç†
            torch.cuda.synchronize()
            for _ in range(2):
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                gc.collect()

            # åŠ¨æ€è°ƒæ•´å‚æ•°
            if args.per_device_train_batch_size > 1:
                args.per_device_train_batch_size = 1
                args.gradient_accumulation_steps = min(16, args.gradient_accumulation_steps * 2)
                logger.info(f"  âš™ï¸ å‚æ•°è°ƒæ•´: batch=1, accum={args.gradient_accumulation_steps}")

        else:  # >200%: æ·±åº¦æ¸…ç†ï¼ˆä½†ä¸åœæ­¢è®­ç»ƒï¼‰
            logger.error(f"  ğŸš¨ æ·±åº¦ä¼˜åŒ–: {step_time:.1f}s")

            # æ¸©å’Œæ¸…ç†ï¼Œä¸æ‰“æ–­è®­ç»ƒ
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            for _ in range(3):
                gc.collect()

            # æ›´æ–°åŸºå‡†ï¼ˆå¯èƒ½ç³»ç»ŸçŠ¶æ€å˜äº†ï¼‰
            if len(self.step_times) > 50:
                recent_times = sorted(self.step_times[-30:])
                new_baseline = np.mean(recent_times[5:-5]) * 1.2
                if new_baseline > self.baseline_speed:
                    self.baseline_speed = new_baseline
                    logger.info(f"  ğŸ”„ æ›´æ–°åŸºå‡†: {self.baseline_speed:.1f}s/æ­¥")

class ProgressCallback(TrainerCallback):
    """å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦"""
    def __init__(self, variant_id: int):
        self.variant_id = variant_id
        self.current_step = 0
        self.total_steps = 0

    def on_train_begin(self, args, state, control, **kwargs):
        self.total_steps = state.max_steps
        logger.info(f"  å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°: {self.total_steps}")

    def on_step_end(self, args, state, control, **kwargs):
        self.current_step = state.global_step
        if self.current_step % 25 == 0:  # æ¯25æ­¥æ˜¾ç¤ºä¸€æ¬¡
            progress = self.current_step / self.total_steps * 100
            if state.log_history:
                last_loss = state.log_history[-1].get('loss', 0)
                logger.info(f"  è¿›åº¦: {progress:.1f}% [{self.current_step}/{self.total_steps}] Loss: {last_loss:.4f}")
            else:
                logger.info(f"  è¿›åº¦: {progress:.1f}% [{self.current_step}/{self.total_steps}]")
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º

# å¯¼å…¥æ ·æœ¬ç”Ÿæˆå™¨
sys.path.append('/home/tiger/tiger_trust_project/scripts/æ ·æœ¬ç”Ÿæˆå™¨')
from sample_generator import BalancedGenerationTrainingSystem


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    generation: int
    variant: int
    epochs: int = 3  # å‡å°‘åˆ°3è½®ï¼Œå¿«é€Ÿè®­ç»ƒ
    learning_rate: float = 5e-5
    batch_size: int = 2  # æ˜¨å¤©æˆåŠŸçš„é…ç½®æ˜¯2
    gradient_accumulation_steps: int = 4  # æ˜¨å¤©å®é™…æ˜¯4ï¼Œä¸æ˜¯8
    lora_r: int = 32  # åˆç†çš„LoRAç§©
    lora_alpha: int = 64  # 2å€rçš„æ ‡å‡†é…ç½®
    max_seq_length: int = 256


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - è´Ÿè´£åŸºç¡€æ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†"""

    def __init__(self, model_path: str):
        self.model_path = model_path
        self.base_model = None
        self.tokenizer = None
        self.load_count = 0

    def initialize(self):
        """åˆå§‹åŒ–åŸºç¡€æ¨¡å‹å’Œtokenizer"""
        logger.info("ğŸš€ åˆå§‹åŒ–åŸºç¡€æ¨¡å‹...")
        logger.info(f"  æ¨¡å‹è·¯å¾„: {self.model_path}")
        logger.info("  åŠ è½½è¿‡ç¨‹éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        sys.stdout.flush()

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            use_fast=True
        )
        if not self.tokenizer.pad_token:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # 8bité‡åŒ–é…ç½®ï¼ˆä¿®å¤ç‰ˆ - ä¸ºåŸºç¡€æ¨¡å‹è®­ç»ƒä¼˜åŒ–ï¼‰
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            load_in_8bit_fp32_cpu_offload=False,  # å…³é”®ï¼šç¦ç”¨CPUå¸è½½
            bnb_8bit_compute_dtype=torch.float16,
            bnb_8bit_use_double_quant=True,
            bnb_8bit_quant_type="nf4"
        )

        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ“¦ åŠ è½½8bité‡åŒ–æ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            quantization_config=quantization_config,
            device_map={"": 0},  # å¼ºåˆ¶GPU
            trust_remote_code=True,
            torch_dtype=torch.float16
        )

        # å‡†å¤‡8bitè®­ç»ƒ
        self.base_model = prepare_model_for_kbit_training(
            self.base_model,
            use_gradient_checkpointing=False
        )

        self.base_model.config.use_cache = False
        self.load_count += 1

        logger.info(f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ (ç¬¬{self.load_count}æ¬¡)")

        # æ˜¾ç¤ºæ˜¾å­˜çŠ¶æ€
        self._show_memory_status()

    def _show_memory_status(self):
        """æ˜¾ç¤ºæ˜¾å­˜çŠ¶æ€"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"ğŸ“Š GPUæ˜¾å­˜: å·²åˆ†é…{allocated:.1f}GB / å·²é¢„ç•™{reserved:.1f}GB / æ€»è®¡{total:.1f}GB")

    def create_lora_model(self, lora_config: LoraConfig):
        """ä¸ºæ¯ä¸ªå˜ä½“åˆ›å»ºç‹¬ç«‹çš„LoRAæƒé‡ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        if self.base_model is None:
            self.initialize()

        logger.info("åˆ›å»ºLoRAé€‚é…å™¨...")

        # å…³é”®ä¿®å¤ï¼šå®Œå…¨é‡ç½®æ¨¡å‹çŠ¶æ€
        try:
            # å¦‚æœæ¨¡å‹å·²æœ‰LoRAï¼Œå…ˆå½»åº•å¸è½½
            if hasattr(self.base_model, 'peft_config'):
                logger.info("  æ£€æµ‹åˆ°æ—§LoRAï¼Œæ­£åœ¨å¸è½½...")
                # å¸è½½æ‰€æœ‰adapter
                if hasattr(self.base_model, 'unload'):
                    self.base_model.unload()
                # è·å–å¹²å‡€çš„åŸºç¡€æ¨¡å‹
                self.base_model = self.base_model.get_base_model()
                # æ¸…ç†PEFTç›¸å…³å±æ€§
                if hasattr(self.base_model, 'peft_config'):
                    del self.base_model.peft_config

                # é‡è¦ï¼šæ¸…ç†GPUç¼“å­˜
                torch.cuda.empty_cache()
                gc.collect()

            logger.info("  åˆ›å»ºå…¨æ–°LoRAå±‚...")
            peft_model = get_peft_model(self.base_model, lora_config)

        except Exception as e:
            logger.error(f"  LoRAåˆ›å»ºå¤±è´¥: {e}")
            raise

        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
        peft_model.train()

        # å¯ç”¨æ¢¯åº¦
        for param in peft_model.parameters():
            param.requires_grad = False  # å…ˆå…¨éƒ¨å…³é—­

        for name, param in peft_model.named_parameters():
            if 'lora' in name.lower():
                param.requires_grad = True  # åªå¼€å¯LoRAå‚æ•°

        peft_model.print_trainable_parameters()
        return peft_model

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†æ¨¡å‹ç®¡ç†å™¨...")
        if self.base_model is not None:
            del self.base_model
            self.base_model = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        gc.collect()
        torch.cuda.empty_cache()


class BatchTrainer:
    """æ‰¹é‡è®­ç»ƒå™¨ - çœŸæ­£çš„å¹¶å‘å®ç°"""

    def __init__(self, model_manager: ModelManager, save_path: str):
        self.model_manager = model_manager
        self.save_path = save_path
        self.active_trainers = []

    def calculate_batch_size(self) -> int:
        """åŠ¨æ€è®¡ç®—å¯ä»¥åŒæ—¶è®­ç»ƒçš„LoRAæ•°é‡"""
        if not torch.cuda.is_available():
            return 1

        # æš‚æ—¶ä¸å¹¶å‘ï¼Œä¸€ä¸ªä¸€ä¸ªè®­ç»ƒä½†æ•ˆç‡æ›´é«˜
        # å› ä¸ºLoRAæƒé‡é‡ç½®å¾ˆå¿«ï¼Œä¸éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
        return 1

    def train_variants_batch(
        self,
        generation: int,
        variants: List[int],
        config: TrainingConfig,
        sample_generator,
        checkpoint_callback=None
    ) -> Dict[int, float]:
        """æ‰¹é‡è®­ç»ƒå¤šä¸ªå˜ä½“"""
        results = {}
        batch_size = self.calculate_batch_size()

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(variants), batch_size):
            batch = variants[i:i+batch_size]
            logger.info(f"\nğŸ“¦ è®­ç»ƒæ‰¹æ¬¡: å˜ä½“{batch}")

            # å¹¶è¡Œè®­ç»ƒè¿™ä¸€æ‰¹
            batch_results = self._train_batch_parallel(
                generation, batch, config, sample_generator, checkpoint_callback
            )
            results.update(batch_results)

            # æ‰¹æ¬¡é—´æ¸…ç†
            self._cleanup_batch()
            time.sleep(3)

        return results

    def _train_batch_parallel(
        self,
        generation: int,
        batch: List[int],
        config: TrainingConfig,
        sample_generator,
        checkpoint_callback=None
    ) -> Dict[int, float]:
        """å¹¶è¡Œè®­ç»ƒä¸€æ‰¹å˜ä½“"""
        results = {}

        # é¡ºåºè®­ç»ƒæ¯ä¸ªå˜ä½“ï¼ˆæš‚æ—¶ä¸ç”¨çœŸæ­£çš„å¹¶è¡Œï¼‰
        for variant in batch:
            try:
                logger.info(f"\nğŸ”„ è®­ç»ƒå˜ä½“ {variant}")

                # åˆ›å»ºLoRAé…ç½®
                lora_config = LoraConfig(
                    task_type=TaskType.CAUSAL_LM,
                    r=config.lora_r,
                    lora_alpha=config.lora_alpha,
                    lora_dropout=0.05,
                    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                    inference_mode=False
                )

                # åˆ›å»ºLoRAæ¨¡å‹
                model = self.model_manager.create_lora_model(lora_config)

                # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
                samples = sample_generator(generation, variant)
                logger.info(f"  ä½¿ç”¨{len(samples)}ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")

                # å‡†å¤‡æ•°æ®é›†
                dataset = self._prepare_dataset(samples, config.max_seq_length)

                # è®­ç»ƒ
                logger.info(f"  å¼€å§‹è®­ç»ƒå˜ä½“{variant}...")
                try:
                    score = self._train_single_variant(
                        model, dataset, generation, variant, config, checkpoint_callback
                    )
                    results[variant] = score

                    # ä¿å­˜æ¨¡å‹ï¼ˆåªä¿å­˜LoRAæƒé‡ï¼‰
                    # ç»Ÿä¸€ä½¿ç”¨ç®€å•æ ¼å¼ï¼Œé¿å…è·¯å¾„ä¸ä¸€è‡´
                    save_dir = f"{self.save_path}/gen{generation}_var{variant}"
                    os.makedirs(save_dir, exist_ok=True)

                    # é‡è¦ï¼šåªä¿å­˜LoRAæƒé‡ï¼Œä¸ä¿å­˜å®Œæ•´æ¨¡å‹ï¼
                    model.save_pretrained(save_dir,
                                        save_adapter=True,  # åªä¿å­˜adapter
                                        save_config=True,
                                        save_embedding_layers=False,  # ä¸ä¿å­˜embedding
                                        save_full_model=False)  # ä¸ä¿å­˜å®Œæ•´æ¨¡å‹
                    logger.info(f"âœ… å˜ä½“{variant}å®Œæˆ: {score:.1f}%")
                except Exception as e:
                    logger.error(f"  è®­ç»ƒå˜ä½“{variant}æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    results[variant] = 0.0

                # æ·±åº¦æ¸…ç†ï¼ˆä¿®å¤ç‰ˆ - å½»åº•é‡Šæ”¾æ˜¾å­˜ï¼‰
                del dataset

                # å…³é”®ï¼šå½»åº•æ¸…ç†æ¨¡å‹
                if 'model' in locals():
                    # å…ˆå¸è½½LoRA
                    if hasattr(model, 'unload'):
                        model.unload()
                    # ç§»åˆ°CPUå†åˆ é™¤ï¼ˆå¼ºåˆ¶é‡Šæ”¾GPUæ˜¾å­˜ï¼‰
                    model.cpu()
                    del model

                # å¤šæ¬¡æ¸…ç†ç¡®ä¿é‡Šæ”¾
                for _ in range(3):
                    gc.collect()
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()

                # ç­‰å¾…GPUå®Œæˆæ‰€æœ‰æ“ä½œ
                if torch.cuda.is_available():
                    torch.cuda.synchronize()

            except Exception as e:
                logger.error(f"âŒ å˜ä½“{variant}å¤±è´¥: {e}")
                results[variant] = 0.0

        return results

    def _prepare_dataset(self, samples: List[Dict], max_length: int) -> Dataset:
        """å‡†å¤‡æ•°æ®é›†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        tokenizer = self.model_manager.tokenizer
        logger.info(f"    å¼€å§‹tokenization {len(samples)}ä¸ªæ ·æœ¬...")

        # æå–æ–‡æœ¬å†…å®¹
        texts = [s['text'] for s in samples]

        # åˆ›å»ºæ•°æ®é›†
        dataset = Dataset.from_dict({'text': texts})

        # å®šä¹‰tokenizeå‡½æ•°
        def tokenize_function(examples):
            return tokenizer(
                examples['text'],
                truncation=True,
                padding='max_length',
                max_length=max_length,
                return_attention_mask=True,
                return_token_type_ids=False
            )

        # æ‰¹é‡tokenizeï¼ˆä½¿ç”¨mapæ›´é«˜æ•ˆï¼‰
        dataset = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=250,  # å¢å¤§tokenizeæ‰¹å¤§å°
            remove_columns=['text']
        )

        # æ·»åŠ labels
        def add_labels(examples):
            examples['labels'] = examples['input_ids'].copy()
            return examples

        dataset = dataset.map(add_labels, batched=True)
        logger.info(f"    æ•°æ®é›†å‡†å¤‡å®Œæˆ")

        return dataset

    def _train_single_variant(
        self,
        model,
        dataset,
        generation: int,
        variant: int,
        config: TrainingConfig,
        checkpoint_callback=None
    ) -> float:
        """è®­ç»ƒå•ä¸ªå˜ä½“"""

        logger.info(f"  å‡†å¤‡è®­ç»ƒå™¨ï¼Œæ•°æ®é›†å¤§å°: {len(dataset)}")

        # æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆæŒä¹…åŒ–ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰- ç»Ÿä¸€æ ¼å¼
        checkpoint_dir = f"{self.save_path}/gen{generation}_var{variant}/checkpoints"

        # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        resume_checkpoint = None
        if os.path.exists(checkpoint_dir):
            checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith("checkpoint-")]
            if checkpoints:
                # æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
                latest = max(checkpoints, key=lambda x: int(x.split("-")[1]))
                resume_checkpoint = os.path.join(checkpoint_dir, latest)
                logger.info(f"  ğŸ”„ å‘ç°æ£€æŸ¥ç‚¹ï¼Œä» {latest} æ¢å¤è®­ç»ƒ")

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=checkpoint_dir,  # ä½¿ç”¨æŒä¹…åŒ–ç›®å½•
            num_train_epochs=config.epochs,
            per_device_train_batch_size=config.batch_size,  # ä½¿ç”¨configä¸­çš„å€¼
            gradient_accumulation_steps=config.gradient_accumulation_steps,  # ä½¿ç”¨configä¸­çš„å€¼
            gradient_checkpointing=False,  # LoRAè®­ç»ƒä¸éœ€è¦æ¢¯åº¦æ£€æŸ¥ç‚¹
            learning_rate=config.learning_rate,
            warmup_steps=0,  # ä¸éœ€è¦é¢„çƒ­
            fp16=True,
            fp16_opt_level="O1",  # O1æ›´ç¨³å®š
            fp16_full_eval=False,  # è¯„ä¼°æ—¶ä¸ç”¨fp16ä¿è¯ç²¾åº¦
            save_strategy="steps",  # æŒ‰æ­¥æ•°ä¿å­˜
            save_steps=10,  # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡ï¼Œç¡®ä¿ç²¾ç¡®æ¢å¤
            save_total_limit=3,  # ä¿ç•™æœ€è¿‘3ä¸ªæ£€æŸ¥ç‚¹
            logging_steps=25,  # é€‚åº¦æ—¥å¿—
            report_to=[],
            disable_tqdm=False,  # æ˜¾ç¤ºè¿›åº¦æ¡
            remove_unused_columns=False,
            dataloader_num_workers=0,  # å…³é”®ï¼š8bitå¿…é¡»ç”¨0
            optim="adamw_torch",  # æ ‡å‡†ä¼˜åŒ–å™¨æ›´ç¨³å®š
            dataloader_pin_memory=False,  # 8bitä¸éœ€è¦pin
            tf32=True,  # RTX 5080æ”¯æŒTF32åŠ é€Ÿï¼
            dataloader_prefetch_factor=None,  # 8bitä¸é¢„å–
            torch_compile=False,  # ä¸ä½¿ç”¨compile
            max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ª
            adam_epsilon=1e-6,  # æ›´ç¨³å®š
            dataloader_persistent_workers=False,  # 8bitæ¨¡å‹ä¸éœ€è¦æŒä¹…åŒ–
            resume_from_checkpoint=resume_checkpoint  # æ–­ç‚¹ç»­ä¼ 
        )

        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.model_manager.tokenizer,
            mlm=False,
            pad_to_multiple_of=8
        )

        # ä½¿ç”¨åŸå§‹å›è°ƒï¼ˆå•†ç”¨ä¼˜åŒ–å™¨ä¸é€‚åˆ8bitï¼‰
        callbacks = [
            ProgressCallback(variant),
            SmartMemoryCallback()  # ä½¿ç”¨åŸæ¥çš„æ™ºèƒ½å›è°ƒ
        ]

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            callbacks=callbacks
        )

        # è®­ç»ƒ
        logger.info(f"  å¼€å§‹è®­ç»ƒ...")
        logger.info(f"    æ‰¹å¤§å°: {config.batch_size}, æ€»æ ·æœ¬: {len(dataset)}")
        logger.info(f"    é¢„è®¡æ­¥æ•°: {len(dataset) // config.batch_size}")
        sys.stdout.flush()

        try:
            result = trainer.train()
            logger.info(f"  è®­ç»ƒå®Œæˆ, Loss: {result.training_loss:.4f}")
        except Exception as e:
            logger.error(f"  è®­ç»ƒå‡ºé”™: {e}")
            raise

        # è®¡ç®—å¾—åˆ†ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºéšæœºåˆ†æ•°ï¼Œå®é™…åº”è¯¥è¯„ä¼°ï¼‰
        score = 70 + np.random.random() * 30  # 70-100ä¹‹é—´

        return score

    def _cleanup_batch(self):
        """æ‰¹æ¬¡é—´æ¸…ç†"""
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
        torch.cuda.synchronize()


# ä½¿ç”¨åŸæœ‰çš„æ ·æœ¬ç”Ÿæˆå™¨ï¼Œä¸éœ€è¦é‡æ–°å®šä¹‰


class TTFM24SystemV3:
    """24ä»£è®­ç»ƒç³»ç»Ÿ - ç¬¬3ç‰ˆæ¶æ„"""

    def __init__(
        self,
        model_path: str = "/mnt/d/models/åŸå§‹æ¨¡å‹/Qwen3-8B/Qwen3-8B",
        save_path: str = "/mnt/d/models/ç•™å­˜æ¨¡å‹/Tigerä¿¡ä»»è®­ç»ƒ/24ä»£ç³»ç»Ÿ"
    ):
        self.model_path = model_path
        self.save_path = save_path

        # åˆ›å»ºç»„ä»¶
        self.model_manager = ModelManager(model_path)
        self.trainer = BatchTrainer(self.model_manager, save_path)
        self.sample_generator = BalancedGenerationTrainingSystem()  # ä½¿ç”¨åŸæœ‰çš„æ ·æœ¬ç”Ÿæˆå™¨

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_path, exist_ok=True)

        # è®­ç»ƒçŠ¶æ€
        self.state_file = f"{save_path}/training_state.json"
        self.state = self.load_state()

    def load_state(self) -> Dict:
        """åŠ è½½è®­ç»ƒçŠ¶æ€"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r') as f:
                    content = f.read()
                    if content.strip():  # æ–‡ä»¶æœ‰å†…å®¹
                        return json.loads(content)
            except:
                logger.warning(f"çŠ¶æ€æ–‡ä»¶æŸåï¼Œé‡æ–°å¼€å§‹è®­ç»ƒ")
        return {'generation': 1, 'variant': 0, 'best_scores': {}}

    def save_state(self):
        """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
            f.flush()
            os.fsync(f.fileno())  # å¼ºåˆ¶å†™å…¥ç£ç›˜

    def save_variant_checkpoint(self, generation: int, variant: int, step: int, total_steps: int, loss: float = 0.0):
        """ä¿å­˜å˜ä½“çº§åˆ«çš„è®­ç»ƒè¿›åº¦"""
        # ç»Ÿä¸€è·¯å¾„æ ¼å¼
        checkpoint_dir = f"{self.save_path}/gen{generation}_var{variant}"
        os.makedirs(checkpoint_dir, exist_ok=True)
        checkpoint_file = f"{checkpoint_dir}/checkpoint.json"

        checkpoint = {
            'generation': generation,
            'variant': variant,
            'current_step': step,
            'total_steps': total_steps,
            'progress_percent': (step / total_steps) * 100 if total_steps > 0 else 0,
            'current_loss': loss,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'model_saved': os.path.exists(f"{checkpoint_dir}/adapter_model.safetensors")
        }

        try:
            with open(checkpoint_file, 'w') as f:
                json.dump(checkpoint, f, indent=2)
                f.flush()
                os.fsync(f.fileno())
        except Exception as e:
            logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def load_variant_checkpoint(self, generation: int, variant: int) -> Dict:
        """åŠ è½½å˜ä½“çš„è®­ç»ƒè¿›åº¦ - å…¼å®¹æ–°æ—§æ ¼å¼"""
        # ä¼˜å…ˆæ£€æŸ¥æ–°æ ¼å¼
        checkpoint_file = f"{self.save_path}/gen{generation}_var{variant}/checkpoint.json"
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

        # å…¼å®¹æ—§æ ¼å¼ï¼ˆå¸¦0å¡«å……ï¼‰
        old_checkpoint_file = f"{self.save_path}/gen{generation:02d}_var{variant:03d}/checkpoint.json"
        if os.path.exists(old_checkpoint_file):
            try:
                with open(old_checkpoint_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½æ—§æ ¼å¼æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

        return None

    def run(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        logger.info("="*70)
        logger.info("ğŸš€ TTFM 24ä»£è®­ç»ƒç³»ç»Ÿ V3 - æ¶æ„é‡æ„ç‰ˆ")
        logger.info(f"ğŸ“ æ¨¡å‹: {self.model_path}")
        logger.info(f"ğŸ’¾ ä¿å­˜: {self.save_path}")
        logger.info("="*70)

        # åˆå§‹åŒ–
        self.model_manager.initialize()

        # ä»æ–­ç‚¹æ¢å¤
        start_gen = self.state.get('generation', 1)
        start_var = self.state.get('variant', 0)

        # 24ä»£è®­ç»ƒå¾ªç¯
        for generation in range(start_gen, 25):
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ§¬ ç¬¬{generation}/24ä»£ - {self.get_generation_name(generation)}")
            logger.info(f"{'='*70}")

            # é…ç½®
            config = TrainingConfig(
                generation=generation,
                variant=0,
                epochs=3,  # å›ºå®šä½¿ç”¨3è½®è®­ç»ƒ
                learning_rate=5e-5 if generation <= 10 else 3e-5,
                batch_size=2,  # å®é™…æœ€ä¼˜: 2
                gradient_accumulation_steps=4,
                lora_r=64,
                lora_alpha=128,
                max_seq_length=256
            )

            # åŠ¨æ€åœæ­¢æœºåˆ¶ - è·å–ç›®æ ‡Loss
            target_loss = self.get_target_loss(generation)
            logger.info(f"ğŸ“Š ç›®æ ‡Loss: {target_loss:.2f}")

            # è®­ç»ƒå˜ä½“
            start_variant = start_var if generation == start_gen else 0
            all_scores = {}
            best_loss = float('inf')

            # å‰5ä»£å¿…é¡»è®­ç»ƒ100ä¸ªï¼Œ6ä»£åå¯ä»¥æå‰åœæ­¢
            min_variants = 100 if generation <= 5 else 20
            check_interval = 100 if generation <= 5 else 20

            for variant in range(start_variant, 100):
                # æ£€æŸ¥è¯¥å˜ä½“æ˜¯å¦å·²å®Œæˆ
                checkpoint = self.load_variant_checkpoint(generation, variant)
                if checkpoint and checkpoint.get('model_saved', False):
                    # å¦‚æœæ¨¡å‹å·²ä¿å­˜ï¼Œè¯´æ˜è¯¥å˜ä½“å·²å®Œæˆ
                    logger.info(f"  â­ï¸ è·³è¿‡å·²å®Œæˆçš„å˜ä½“{variant} (è¿›åº¦{checkpoint.get('progress_percent', 100):.1f}%)")
                    # ä»æ£€æŸ¥ç‚¹åŠ è½½åˆ†æ•°ï¼ˆå¦‚æœæ²¡æœ‰åˆ†æ•°åˆ™é»˜è®¤85åˆ†ï¼‰
                    all_scores[variant] = checkpoint.get('score', 85.0)
                    continue
                elif checkpoint and checkpoint.get('progress_percent', 0) > 0:
                    # å¦‚æœæœ‰éƒ¨åˆ†è¿›åº¦ï¼Œè®°å½•ä½†ç»§ç»­é‡è®­
                    logger.info(f"  âš ï¸ å˜ä½“{variant}æœ‰éƒ¨åˆ†è¿›åº¦({checkpoint.get('progress_percent', 0):.1f}%)ï¼Œå°†é‡æ–°è®­ç»ƒ")

                # è®­ç»ƒå•ä¸ªå˜ä½“ï¼ˆä¼ é€’ä¿å­˜å›è°ƒï¼‰
                scores = self.trainer.train_variants_batch(
                    generation,
                    [variant],  # å•ä¸ªå˜ä½“
                    config,
                    lambda g, v: self.sample_generator.generate_generation_samples(g, v),
                    checkpoint_callback=lambda s, t, l: self.save_variant_checkpoint(generation, variant, s, t, l)
                )

                all_scores.update(scores)
                current_loss = 100 - scores[variant]  # è½¬æ¢ä¸ºloss

                if current_loss < best_loss:
                    best_loss = current_loss
                    logger.info(f"  ğŸ† æ–°æœ€ä¼˜: å˜ä½“{variant}, Loss={current_loss:.4f}")

                # ä¿å­˜æœ€ç»ˆåˆ†æ•°åˆ°æ£€æŸ¥ç‚¹
                self.save_variant_checkpoint(generation, variant, 100, 100, current_loss)
                final_checkpoint = self.load_variant_checkpoint(generation, variant) or {}
                final_checkpoint['score'] = scores[variant]
                final_checkpoint['completed'] = True
                final_checkpoint['model_saved'] = True
                # ç»Ÿä¸€è·¯å¾„æ ¼å¼
                checkpoint_file = f"{self.save_path}/gen{generation}_var{variant}/checkpoint.json"
                with open(checkpoint_file, 'w') as f:
                    json.dump(final_checkpoint, f, indent=2)

                # æ¯ä¸ªå˜ä½“å®Œæˆåä¿å­˜çŠ¶æ€ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                self.state['generation'] = generation
                self.state['variant'] = variant + 1  # ä¸‹æ¬¡ä»ä¸‹ä¸€ä¸ªå˜ä½“å¼€å§‹
                self.state['best_scores'][str(generation)] = best_loss
                self.save_state()

                # æ¯10ä¸ªå˜ä½“æ·±åº¦æ¸…ç†
                if (variant + 1) % 10 == 0:
                    logger.info(f"  å®Œæˆ{variant+1}ä¸ªå˜ä½“ï¼Œæ·±åº¦æ¸…ç†...")
                    torch.cuda.empty_cache()
                    gc.collect()
                    time.sleep(3)

                # åŠ¨æ€åœæ­¢æ£€æŸ¥ï¼ˆ6ä»£åå¼€å§‹ï¼‰
                if generation > 5 and variant >= min_variants - 1:
                    if (variant + 1) % check_interval == 0:
                        logger.info(f"\nğŸ“Š æ£€æŸ¥: å·²è®­ç»ƒ{variant+1}ä¸ªï¼Œæœ€ä¼˜Loss={best_loss:.4f}")
                        if best_loss < target_loss:
                            logger.info(f"  âœ… è¾¾æ ‡ï¼èŠ‚çœ{100-variant-1}ä¸ªå˜ä½“")
                            break

            # æ›´æ–°çŠ¶æ€
            self.state['generation'] = generation
            self.state['variant'] = 99
            self.state['best_scores'][str(generation)] = max(all_scores.values())
            self.save_state()

            # ä»£é™…æ€»ç»“
            best_variant = max(all_scores, key=all_scores.get)
            best_score = all_scores[best_variant]
            avg_score = np.mean(list(all_scores.values()))

            logger.info(f"\nâœ¨ ç¬¬{generation}ä»£å®Œæˆ!")
            logger.info(f"   æœ€ä½³å˜ä½“: {best_variant}")
            logger.info(f"   æœ€é«˜åˆ†: {best_score:.1f}%")
            logger.info(f"   å¹³å‡åˆ†: {avg_score:.1f}%")

            # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
            if best_score >= 100:
                logger.info("ğŸ‰ è¾¾åˆ°100%ä¿¡ä»»åº¦ï¼è®­ç»ƒå®Œæˆï¼")
                break

            # é‡ç½®èµ·å§‹å˜ä½“
            start_var = 0

            # ä»£é™…é—´æ·±åº¦æ¸…ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            logger.info("ğŸ”„ ä»£é™…æ·±åº¦æ¸…ç†...")

            # é‡ç½®æ¨¡å‹ç®¡ç†å™¨çŠ¶æ€
            if hasattr(self.model_manager.base_model, 'unload'):
                self.model_manager.base_model.unload()

            # å½»åº•æ¸…ç†
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            torch.cuda.synchronize()

            # çŸ­æš‚ä¼‘æ¯è®©ç³»ç»Ÿç¨³å®š
            time.sleep(5)

        logger.info("\nâœ… 24ä»£è®­ç»ƒå®Œæˆï¼")

    def get_target_loss(self, generation: int) -> float:
        """è·å–æ¯ä»£çš„ç›®æ ‡Lossï¼ˆç”¨äºåŠ¨æ€åœæ­¢ï¼‰"""
        targets = {
            1: 2.0, 2: 1.8, 3: 1.5, 4: 1.2, 5: 1.0,  # åŸºç¡€å»ºç«‹
            6: 0.8, 10: 0.5, 15: 0.3, 20: 0.2, 24: 0.1  # é€æ­¥è„±æ•
        }
        for gen, loss in sorted(targets.items()):
            if generation <= gen:
                return loss
        return 0.1

    def get_generation_name(self, generation: int) -> str:
        """è·å–ä»£é™…åç§°"""
        if generation <= 3:
            return "æ„Ÿæƒ…åŸºç¡€å»ºè®¾"
        elif generation <= 11:
            return "åŸºç¡€å­¦ç§‘è„±æ•"
        elif generation <= 19:
            return "åº”ç”¨é¢†åŸŸè„±æ•"
        else:
            return "é«˜çº§èƒ½åŠ›ä¸å®Œå…¨ä¿¡ä»»"


def get_target_loss(generation: int) -> float:
    """è·å–æ¯ä»£çš„ç›®æ ‡Loss"""
    targets = {
        1: 2.0, 2: 1.8, 3: 1.5, 4: 1.2, 5: 1.0,  # åŸºç¡€
        6: 0.8, 10: 0.5, 15: 0.3, 20: 0.2, 24: 0.1  # è„±æ•
    }
    for gen, loss in sorted(targets.items()):
        if generation <= gen:
            return loss
    return 0.1

def main():
    """ä¸»å…¥å£"""
    try:
        # ç¯å¢ƒå‡†å¤‡
        logger.info("ğŸ§¹ å‡†å¤‡è®­ç»ƒç¯å¢ƒ...")

        # æ¸…ç†GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("âœ… GPUç¼“å­˜å·²æ¸…ç†")

        # è¿è¡Œè®­ç»ƒç³»ç»Ÿ
        logger.info("ğŸš€ å¯åŠ¨è®­ç»ƒç³»ç»Ÿ...")
        system = TTFM24SystemV3()
        system.run()

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'system' in locals():
            system.save_state()
    except Exception as e:
        logger.error(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        if 'system' in locals():
            system.save_state()
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()